{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T08:19:54.819195Z",
     "start_time": "2024-05-20T08:19:54.816934Z"
    }
   },
   "source": [
    "# Import\n",
    "import os\n",
    "import kaggle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T08:19:54.822892Z",
     "start_time": "2024-05-20T08:19:54.820327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ************** CONSTANTS ************* #\n",
    "\n",
    "\n",
    "# PATHS\n",
    "DATASET_PATH = \"archives\"  # Path to the dataset\n",
    "DATASET_ID = \"plameneduardo/sarscov2-ctscan-dataset\"  # ID for Kaggle API. Format: dataset_owner_name/dataset_name\n",
    "COVID_PATH = os.path.join(DATASET_PATH, \"COVID\")  # Path to data labeled as COVID\n",
    "NON_COVID_PATH = os.path.join(DATASET_PATH, \"non-COVID\")  # Path to data labeled as non-COVID\n",
    "DATA_PATH = \"data\"  # Path to the data folder \n",
    "PLOT_PATH = \"plot\"  # Path to the plot folder\n",
    "\n",
    "# DATA RELATED\n",
    "CLASSES = [\"COVID\", \"non-COVID\"]\n",
    "BASELINE = \"baseline_system\"\n",
    "\n",
    "# SIMILARITY RELATED\n",
    "TOP_N = 100  # Number of images retrieved\n",
    "K_VALUES = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # K ranking values\n"
   ],
   "id": "44a588eba13acb1d",
   "outputs": [],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "id": "a07532c2f38321a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T08:19:54.826391Z",
     "start_time": "2024-05-20T08:19:54.823847Z"
    }
   },
   "source": [
    "# Download dataset from Kaggle website\n",
    "def download_dataset_from_kaggle(dataset_id: str, dataset_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Download dataset using Kaggle module\n",
    "\n",
    "    :param dataset_id: identify the dataset to download.\n",
    "        Format: dataset_owner_name/dataset_name\n",
    "    :param dataset_path: location to save the dataset\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Download the dataset if not exist in the workplace\n",
    "    if not os.path.exists(path=dataset_path):\n",
    "        \n",
    "        print(\"\\n> Download the dataset from Kaggle...\")\n",
    "        # Download dataset and unzip it\n",
    "        kaggle.api.dataset_download_files(dataset=dataset_id, path=dataset_path, quiet=False, unzip=True)\n",
    "    else:\n",
    "        print(\"\\n> Dataset already downloaded.\")\n",
    "        \n",
    "        \n",
    "## ************************************ DOWNLOAD THE DATASET ********************************** ##        \n",
    "\n",
    "\n",
    "# Execute the download operation\n",
    "download_dataset_from_kaggle(dataset_id=DATASET_ID, dataset_path=DATASET_PATH)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Dataset already downloaded.\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "id": "ab0fa0a3ebe89d99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T08:21:52.832817Z",
     "start_time": "2024-05-20T08:21:52.825240Z"
    }
   },
   "source": [
    "# Define a pandas dataframe\n",
    "def build_dataframe(dataset_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a dataframe from dataset\n",
    "    \n",
    "    :param dataset_path: Path to the dataset\n",
    "    \n",
    "    :return: Dataframe.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for class_label, directory_names in enumerate(CLASSES):\n",
    "        # Folder path -> archives/COVID or archives/non-COVID\n",
    "        folder_path = os.path.join(dataset_path, directory_names)  \n",
    "        \n",
    "        for file in os.listdir(folder_path):\n",
    "            # File path -> example. archives/COVID/Covid (1).png\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            \n",
    "            # Append file information along with class label to the data list\n",
    "            data.append([file_path, class_label])\n",
    "            \n",
    "    # Create a DataFrame using the collected data with appropriate column name\n",
    "    data_df = pd.DataFrame(data=data, columns=[\"file_paths\", \"labels\"])\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "\n",
    "# Return n random indices from train dataset\n",
    "def get_random_indices(train_dataframe: pd.DataFrame, top_n: int=TOP_N) -> list:\n",
    "    \"\"\"\n",
    "    Return N random indices from the train set\n",
    "    \n",
    "    :param train_dataframe: DataFrame containing the data.\n",
    "    :param top_n: Number of random indices to return.\n",
    "    \n",
    "    :return: List of N random indices\n",
    "    \"\"\"\n",
    "    return train_dataframe.sample(n=top_n).index.tolist()\n",
    "\n",
    "\n",
    "# Evaluation of the model performance\n",
    "def compute_evaluation_metrics(query_img_label: int, image_labels_retrieved: np.ndarray[int], k: int) -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the similarity between the query image and the Top N most similar images.\n",
    "\n",
    "    :param query_img_label: Label of the query image.\n",
    "    :param_ image_labels_retrieved: Labels of the Top-N most similar images retrieved.\n",
    "    :param k: The number of top items to consider.\n",
    "\n",
    "    :return: precision, recall, F1-score.\n",
    "    \"\"\"\n",
    "    # Count of the Relevant K items\n",
    "    k_relevant_retrieved = np.sum(image_labels_retrieved[:k] == query_img_label)\n",
    "    # Compute Precision score\n",
    "    precision_score = k_relevant_retrieved / k\n",
    "    \n",
    "    # Count of the All Relevant items in the Top N images retrieved\n",
    "    total_relevant_items = np.sum(image_labels_retrieved == query_img_label)\n",
    "    # Compute Recall score \n",
    "    recall_score = k_relevant_retrieved / total_relevant_items if total_relevant_items != 0 else 0.0\n",
    "    \n",
    "    # Compute F1-score\n",
    "    if precision_score + recall_score > 0.0:\n",
    "        f1_score = 2 * (precision_score * recall_score) / (precision_score + recall_score)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "    \n",
    "    # Relevance scores for DCG (1 for relevant, 0 for non-relevant)\n",
    "    relevance_scores = np.array(image_labels_retrieved == query_img_label)[:k]\n",
    "    # Compute DCG\n",
    "    dcg = np.sum(relevance_scores / np.log2(np.arange(2, relevance_scores.size + 2))) if relevance_scores.size != 0 else 0.0 \n",
    "\n",
    "    return precision_score, recall_score, f1_score, dcg\n",
    "\n",
    "\n",
    "# Evaluation performed on all the element of the test set\n",
    "def get_average_model_performance(query_dataset: pd.DataFrame, training_dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate the test dataset by computing precision, recall, and F1-score and output the average values.\n",
    "\n",
    "    :param query_dataset: DataFrame containing the test dataset.\n",
    "    :param training_dataset: DataFrame containing the training dataset.\n",
    "\n",
    "    :return: average precision, average recall, and  average f1-score.\n",
    "    \"\"\"\n",
    "    # Dictionary to collect data\n",
    "    metrics = {k: {\"precision\": [], \"recall\": [], \"f1\": [], \"dcg\": []} for k in K_VALUES}\n",
    "    \n",
    "    print(\"\\n> Computing evaluation metrics for each test sample...\\n\")\n",
    "    # Iterate through each feature in the test dataset\n",
    "    for query_file_path, query_label in zip(query_dataset[\"file_paths\"], query_dataset[\"labels\"]):\n",
    "        # Print query name\n",
    "        query_file_name = os.path.splitext(os.path.basename(query_file_path))[0]\n",
    "        print(f\"\\n-- Processing: {os.path.basename(query_file_name)} - label: {query_label}\")\n",
    "        \n",
    "        # Retrieve random images \n",
    "        similar_index = get_random_indices(train_dataframe=training_dataset)\n",
    "        \n",
    "        # Print retrieved random files name\n",
    "        retrieved_files = training_dataset.loc[similar_index][\"file_paths\"]\n",
    "        retrieved_file_names = [os.path.splitext(os.path.basename(file))[0] for file in retrieved_files]\n",
    "        print(f\"-- Images retrieved: {\", \".join(retrieved_file_names)}\")\n",
    "\n",
    "        # Extract labels of similar images\n",
    "        similar_label = training_dataset.loc[similar_index][\"labels\"].to_numpy()\n",
    "        print(f\"-- Labels: {similar_label}\")\n",
    "        \n",
    "        # Loop through K\n",
    "        for k in K_VALUES:\n",
    "            # Compute precision, recall, and F1-score\n",
    "            precision, recall, f1, dcg = compute_evaluation_metrics(query_img_label=query_label, image_labels_retrieved=similar_label, k=k)\n",
    "            # Append scores to Dictionary\n",
    "            metrics[k][\"precision\"].append(precision)\n",
    "            metrics[k][\"recall\"].append(recall)\n",
    "            metrics[k][\"f1\"].append(f1)\n",
    "            metrics[k][\"dcg\"].append(dcg)\n",
    "        \n",
    "    print(\"\\n> Process completed!\")\n",
    "\n",
    "    # Compute metrics average and build a dataframe\n",
    "    avg_metrics = {k: {\"avg_precision\": np.mean(metrics[k][\"precision\"]),\n",
    "                       \"avg_recall\": np.mean(metrics[k][\"recall\"]),\n",
    "                       \"avg_f1\": np.mean(metrics[k][\"f1\"]),\n",
    "                       \"avg_dcg\": np.mean(metrics[k][\"dcg\"])}\n",
    "                   for k in K_VALUES}\n",
    "    df_avg_metrics = pd.DataFrame.from_dict(avg_metrics, orient=\"index\")\n",
    "    df_avg_metrics.index.name = \"K\"\n",
    "    \n",
    "    # Save the Dataframe as CSV\n",
    "    metrics_path = os.path.join(DATA_PATH, f\"{BASELINE}_evaluation.csv\")\n",
    "    df_avg_metrics.to_csv(path_or_buf=metrics_path, float_format=\"%.4f\")\n",
    "\n",
    "    return df_avg_metrics\n"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "id": "e815565c51b60732",
   "metadata": {},
   "source": [
    "# ******************************* BASELINE SYSTEM - PERFORMANCE EVALUATION ***************************** ##\n",
    "\n",
    "\n",
    "# Build the dataframe \n",
    "dataset_df = build_dataframe(dataset_path=DATASET_PATH)\n",
    "\n",
    "# Splitting dataset into Training and Test\n",
    "train_df, test_df = train_test_split(dataset_df, test_size=0.2, shuffle=True, random_state=4)\n",
    "\n",
    "# Compute Average Classification metrics values\n",
    "df = get_average_model_performance(\n",
    "    query_dataset=test_df, \n",
    "    training_dataset=train_df\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "36f27e6ae310c379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-20T08:50:04.714453Z",
     "start_time": "2024-05-20T08:50:04.711691Z"
    }
   },
   "source": [
    "# Print average performance\n",
    "print(f\"\\n> BASELINE SYSTEM AVERAGE PERFORMANCE:\")\n",
    "print(df.to_string())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> BASELINE SYSTEM AVERAGE PERFORMANCE:\n",
      "     avg_precision  avg_recall    avg_f1    avg_dcg\n",
      "K                                                  \n",
      "5         0.502616    0.050192  0.091195   1.489020\n",
      "10        0.508451    0.101639  0.169187   2.311265\n",
      "20        0.500805    0.199851  0.285140   3.541912\n",
      "30        0.500671    0.299383  0.373860   4.602720\n",
      "40        0.503924    0.401663  0.445951   5.593869\n",
      "50        0.500523    0.498688  0.498396   6.473602\n",
      "60        0.502515    0.601137  0.546111   7.355093\n",
      "70        0.502472    0.700965  0.583977   8.184723\n",
      "80        0.501836    0.800460  0.615501   8.979630\n",
      "90        0.500872    0.898853  0.641864   9.746145\n",
      "100       0.501469    1.000000  0.666546  10.515106\n"
     ]
    }
   ],
   "execution_count": 96
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
