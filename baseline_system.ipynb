{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Information Retrieval Project - Baseline System\n",
    "\n",
    "The code below represents a baseline system serving as a comparison point for the other adopted implementations.\n"
   ],
   "id": "caa7ecb3b6243386"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Installation of Requirements\n",
    "\n",
    "This part of the code installs necessary dependencies specified in the `requirements.txt` file.\n",
    "Utilizes the `!pip install -r requirements.txt` command to install packages listed in the `requirements.txt` file. This ensures that all required dependencies are installed before proceeding with execution.\n"
   ],
   "id": "64eaafd9ffb9a5b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install -r requirements.txt",
   "id": "1fad65ab50caf320"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Importing Required Libraries\n",
    "\n",
    "This part of the code imports necessary libraries."
   ],
   "id": "44828b062b510635"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T09:01:41.287451Z",
     "start_time": "2024-05-21T09:01:35.931992Z"
    }
   },
   "source": [
    "# Import\n",
    "import os\n",
    "import kaggle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Definition of constants\n",
    "\n",
    "This part of the code defines useful constants and paths.\n"
   ],
   "id": "94ec4d4faa5971be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T09:01:41.292119Z",
     "start_time": "2024-05-21T09:01:41.288559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ************** CONSTANTS ************* #\n",
    "\n",
    "\n",
    "# PATHS\n",
    "DATASET_PATH = \"archives\"  # Path to the dataset\n",
    "DATASET_ID = \"plameneduardo/sarscov2-ctscan-dataset\"  # ID for Kaggle API. Format: dataset_owner_name/dataset_name\n",
    "COVID_PATH = os.path.join(DATASET_PATH, \"COVID\")  # Path to data labeled as COVID\n",
    "NON_COVID_PATH = os.path.join(DATASET_PATH, \"non-COVID\")  # Path to data labeled as non-COVID\n",
    "DATA_PATH = \"data\"  # Path to the data folder \n",
    "PLOT_PATH = \"plot\"  # Path to the plot folder\n",
    "\n",
    "# DATA RELATED\n",
    "CLASSES = [\"COVID\", \"non-COVID\"]\n",
    "BASELINE = \"baseline_system\"\n",
    "\n",
    "# SIMILARITY RELATED\n",
    "TOP_N = 100  # Number of images retrieved\n",
    "K_VALUES = [5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]  # K ranking values\n"
   ],
   "id": "44a588eba13acb1d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Dataset Download Function**:\n",
    "    - `download_dataset_from_kaggle()` checks if the dataset exists locally; if not, it downloads and unzips it using the Kaggle API."
   ],
   "id": "cf63932b73b9bd63"
  },
  {
   "cell_type": "code",
   "id": "a07532c2f38321a5",
   "metadata": {},
   "source": [
    "# Download dataset from Kaggle website\n",
    "def download_dataset_from_kaggle(dataset_id: str, dataset_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Download dataset using Kaggle module\n",
    "\n",
    "    :param dataset_id: identify the dataset to download.\n",
    "        Format: dataset_owner_name/dataset_name\n",
    "    :param dataset_path: location to save the dataset\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Download the dataset if not exist in the workplace\n",
    "    if not os.path.exists(path=dataset_path):\n",
    "        \n",
    "        print(\"\\n> Download the dataset from Kaggle...\")\n",
    "        # Download dataset and unzip it\n",
    "        kaggle.api.dataset_download_files(dataset=dataset_id, path=dataset_path, quiet=False, unzip=True)\n",
    "    else:\n",
    "        print(\"\\n> Dataset already downloaded.\")\n",
    "        \n",
    "        \n",
    "## ************************************ DOWNLOAD THE DATASET ********************************** ##        \n",
    "\n",
    "\n",
    "# Execute the download operation\n",
    "download_dataset_from_kaggle(dataset_id=DATASET_ID, dataset_path=DATASET_PATH)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**DataFrame Construction**:\n",
    "- `build_dataframe()` creates a DataFrame from the dataset, with file paths and corresponding class labels.\n",
    "\n",
    "**Random Index Selection**:\n",
    "- `get_random_indices()` returns a specified number of random indices from the training dataset.\n",
    "\n",
    "**Evaluation Metrics Calculation**:\n",
    "- `compute_evaluation_metrics()` computes precision, recall, F1-score, and Discounted Cumulative Gain (DCG) based on the labels of the retrieved images compared to the query image's label.\n",
    "\n",
    "**Average Model Performance Evaluation**:\n",
    "- `get_average_model_performance()` iterates over the test dataset, retrieves random images from the training set, and computes the average precision, recall, F1-score, and DCG for various values of `k`.\n",
    "\n",
    "**Execution**:\n",
    "- The dataset is downloaded and a DataFrame is built from the images.\n",
    "- The dataset is split into training and test sets.\n",
    "- The baseline system's performance is evaluated by comparing the retrieved images with the query images.\n",
    "- The average performance metrics are printed."
   ],
   "id": "a761b6f89df2a54c"
  },
  {
   "cell_type": "code",
   "id": "ab0fa0a3ebe89d99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T09:01:41.305343Z",
     "start_time": "2024-05-21T09:01:41.296791Z"
    }
   },
   "source": [
    "# Define a pandas dataframe\n",
    "def build_dataframe(dataset_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a dataframe from dataset.\n",
    "    \n",
    "    :param dataset_path: Path to the dataset directory.\n",
    "    \n",
    "    :return: DataFrame containing file paths and corresponding class labels.\n",
    "    \"\"\"\n",
    "    # Generate a list of tuples containing file paths and their corresponding class labels\n",
    "    data = [\n",
    "        (os.path.join(dataset_path, directory_name, file), class_label)\n",
    "        for class_label, directory_name in enumerate(CLASSES)\n",
    "        for file in os.listdir(os.path.join(dataset_path, directory_name))\n",
    "    ]\n",
    "    # Create a DataFrame from the list of tuples with appropriate column names\n",
    "    data_df = pd.DataFrame(data, columns=[\"file_paths\", \"labels\"])\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "\n",
    "# Return n random indices from train dataset\n",
    "def get_random_indices(train_dataframe: pd.DataFrame, top_n: int=TOP_N) -> list:\n",
    "    \"\"\"\n",
    "    Return N random indices from the train set\n",
    "    \n",
    "    :param train_dataframe: DataFrame containing the data.\n",
    "    :param top_n: Number of random indices to return.\n",
    "    \n",
    "    :return: List of N random indices\n",
    "    \"\"\"\n",
    "    return train_dataframe.sample(n=top_n).index.tolist()\n",
    "\n",
    "\n",
    "# Evaluation of the model performance\n",
    "def compute_evaluation_metrics(query_img_label: int, image_labels_retrieved: np.ndarray[int], k: int) -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the similarity between the query image and the Top N most similar images.\n",
    "\n",
    "    :param query_img_label: Label of the query image.\n",
    "    :param_ image_labels_retrieved: Labels of the Top-N most similar images retrieved.\n",
    "    :param k: The number of top items to consider.\n",
    "\n",
    "    :return: precision, recall, F1-score.\n",
    "    \"\"\"\n",
    "    # Count of the Relevant K items\n",
    "    k_relevant_retrieved = np.sum(image_labels_retrieved[:k] == query_img_label)\n",
    "    # Compute Precision score\n",
    "    precision_score = k_relevant_retrieved / k\n",
    "    \n",
    "    # Count of the All Relevant items in the Top N images retrieved\n",
    "    total_relevant_items = np.sum(image_labels_retrieved == query_img_label)\n",
    "    # Compute Recall score \n",
    "    recall_score = k_relevant_retrieved / total_relevant_items if total_relevant_items != 0 else 0.0\n",
    "    \n",
    "    # Compute F1-score\n",
    "    if precision_score + recall_score > 0.0:\n",
    "        f1_score = 2 * (precision_score * recall_score) / (precision_score + recall_score)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "    \n",
    "    # Relevance scores for DCG (1 for relevant, 0 for non-relevant)\n",
    "    relevance_scores = np.array(image_labels_retrieved == query_img_label)[:k]\n",
    "    # Compute DCG\n",
    "    dcg = np.sum(relevance_scores / np.log2(np.arange(2, relevance_scores.size + 2))) if relevance_scores.size != 0 else 0.0 \n",
    "\n",
    "    return precision_score, recall_score, f1_score, dcg\n",
    "\n",
    "\n",
    "# Evaluation performed on all the element of the test set\n",
    "def get_average_model_performance(query_dataset: pd.DataFrame, training_dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate the test dataset by computing precision, recall, and F1-score and output the average values.\n",
    "\n",
    "    :param query_dataset: DataFrame containing the test dataset.\n",
    "    :param training_dataset: DataFrame containing the training dataset.\n",
    "\n",
    "    :return: average precision, average recall, and  average f1-score.\n",
    "    \"\"\"\n",
    "    # Dictionary to collect data\n",
    "    metrics = {k: {\"precision\": [], \"recall\": [], \"f1\": [], \"dcg\": []} for k in K_VALUES}\n",
    "    \n",
    "    print(\"\\n> Computing evaluation metrics for each test sample...\\n\")\n",
    "    # Iterate through each feature in the test dataset\n",
    "    for query_file_path, query_label in zip(query_dataset[\"file_paths\"], query_dataset[\"labels\"]):\n",
    "        # Print query name\n",
    "        query_file_name = os.path.splitext(os.path.basename(query_file_path))[0]\n",
    "        print(f\"\\n-- Processing: {os.path.basename(query_file_name)} - label: {query_label}\")\n",
    "        \n",
    "        # Retrieve random images \n",
    "        similar_index = get_random_indices(train_dataframe=training_dataset)\n",
    "        \n",
    "        # Print retrieved random files name\n",
    "        retrieved_files = training_dataset.loc[similar_index][\"file_paths\"]\n",
    "        retrieved_file_names = [os.path.splitext(os.path.basename(file))[0] for file in retrieved_files]\n",
    "        print(f\"-- Images retrieved: {\", \".join(retrieved_file_names)}\")\n",
    "\n",
    "        # Extract labels of similar images\n",
    "        similar_label = training_dataset.loc[similar_index][\"labels\"].to_numpy()\n",
    "        print(f\"-- Labels: {similar_label}\")\n",
    "        \n",
    "        # Loop through K\n",
    "        for k in K_VALUES:\n",
    "            # Compute precision, recall, and F1-score\n",
    "            precision, recall, f1, dcg = compute_evaluation_metrics(query_img_label=query_label, image_labels_retrieved=similar_label, k=k)\n",
    "            # Append scores to Dictionary\n",
    "            metrics[k][\"precision\"].append(precision)\n",
    "            metrics[k][\"recall\"].append(recall)\n",
    "            metrics[k][\"f1\"].append(f1)\n",
    "            metrics[k][\"dcg\"].append(dcg)\n",
    "        \n",
    "    print(\"\\n> Process completed!\")\n",
    "\n",
    "    # Compute metrics average and build a dataframe\n",
    "    avg_metrics = {k: {\"avg_precision\": np.mean(metrics[k][\"precision\"]),\n",
    "                       \"avg_recall\": np.mean(metrics[k][\"recall\"]),\n",
    "                       \"avg_f1\": np.mean(metrics[k][\"f1\"]),\n",
    "                       \"avg_dcg\": np.mean(metrics[k][\"dcg\"])}\n",
    "                   for k in K_VALUES}\n",
    "    df_avg_metrics = pd.DataFrame.from_dict(avg_metrics, orient=\"index\")\n",
    "    df_avg_metrics.index.name = \"K\"\n",
    "    \n",
    "    # Save the Dataframe as CSV\n",
    "    metrics_path = os.path.join(DATA_PATH, f\"{BASELINE}_evaluation.csv\")\n",
    "    df_avg_metrics.to_csv(path_or_buf=metrics_path, float_format=\"%.4f\")\n",
    "\n",
    "    return df_avg_metrics\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "e815565c51b60732",
   "metadata": {},
   "source": [
    "# ******************************* BASELINE SYSTEM - PERFORMANCE EVALUATION ***************************** ##\n",
    "\n",
    "\n",
    "# Build the dataframe \n",
    "dataset_df = build_dataframe(dataset_path=DATASET_PATH)\n",
    "\n",
    "# Splitting dataset into Training and Test\n",
    "train_df, test_df = train_test_split(dataset_df, test_size=0.2, shuffle=True, random_state=4)\n",
    "\n",
    "# Compute Average Classification metrics values\n",
    "df = get_average_model_performance(\n",
    "    query_dataset=test_df, \n",
    "    training_dataset=train_df\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "36f27e6ae310c379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T09:02:04.403846Z",
     "start_time": "2024-05-21T09:02:04.400719Z"
    }
   },
   "source": [
    "# Print average performance\n",
    "print(f\"\\n> BASELINE SYSTEM AVERAGE PERFORMANCE:\")\n",
    "print(df.to_string())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> BASELINE SYSTEM AVERAGE PERFORMANCE:\n",
      "     avg_precision  avg_recall    avg_f1    avg_dcg\n",
      "K                                                  \n",
      "5         0.471630    0.047543  0.086305   1.394764\n",
      "10        0.481288    0.097100  0.161371   2.176542\n",
      "20        0.494668    0.199354  0.283620   3.445352\n",
      "30        0.493159    0.297944  0.370630   4.485812\n",
      "40        0.494416    0.398063  0.439998   5.446419\n",
      "50        0.497304    0.500408  0.497664   6.365891\n",
      "60        0.496244    0.599352  0.541657   7.209804\n",
      "70        0.496465    0.699727  0.579482   8.032058\n",
      "80        0.496756    0.799953  0.611526   8.829484\n",
      "90        0.496982    0.900452  0.639063   9.604945\n",
      "100       0.496821    1.000000  0.662421  10.356471\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
