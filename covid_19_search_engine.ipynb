{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Information Retrieval Project\n",
    "## COVID-19 Search Engine (P12)\n",
    "\n",
    "Content-based image retrieval (CBIR) is a computer vision technique which addresses the problem of searching for digital images in large databases. A content-based approach exploits the contents of an image, such as colors, shapes and textures, differing from its concept-based counterpart, which instead focuses on keywords and tags associated with the image itself.\n",
    "\n",
    "Image retrieval has gained more and more relevance in the medical field, due to the accumulation of extensive collections of scans in hospitals. These images are stored in DICOM format, which must be manually annotated and may require considerable time to process by physicians. The goal of this project is trying to address this problem by considering different approaches for building a content-based medical image retrieval system and comparing their results based on classification metrics and computational time."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db3c1946c8dce761"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Installation of Requirements\n",
    "\n",
    "This part of the code installs necessary dependencies specified in the `requirements.txt` file.\n",
    "Utilizes the `!pip install -r requirements.txt` command to install packages listed in the `requirements.txt` file. This ensures that all required dependencies are installed before proceeding with execution.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83f7612f7663945"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb03e351d14986a2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing Required Libraries and Modules Summary\n",
    "\n",
    "This part of the code imports necessary libraries and modules for further execution, providing functionalities for dataset management, visualization, and image processing.\n",
    "\n",
    "## Imported Libraries and Modules\n",
    "- `os`: A standard library for interacting with the operating system.\n",
    "- `kaggle`: A library for accessing the Kaggle API to download datasets.\n",
    "- `platform`: A standard library for accessing platform-specific information.\n",
    "- `imagehash`: A library for computing perceptual image hashes, useful for identifying duplicate images.\n",
    "- `matplotlib.pyplot`: A module from the Matplotlib library used for visualizing images and plotting graphs.\n",
    "- `PIL.Image`: A module from the Python Imaging Library (PIL) used for opening, manipulating, and saving many different image file formats.\n",
    "- `UnidentifiedImageError`: An exception raised when an image file cannot be identified or opened by PIL."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67677eb828dc5627"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import kaggle\n",
    "import platform\n",
    "import imagehash\n",
    "import matplotlib\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, UnidentifiedImageError"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T10:24:14.969142Z",
     "start_time": "2024-02-26T10:24:14.963051Z"
    }
   },
   "id": "ef963128ebd2163f",
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Python and Library Version Checking\n",
    "\n",
    "This part of the code checks the versions of Python and specific libraries installed in the environment.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c281a9d21f536f8"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-26T10:24:58.119029Z",
     "start_time": "2024-02-26T10:24:57.972889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Python version: 3.12.0\n",
      "- Matplotlib version is: 3.8.3\n",
      "- Pillow version is: 10.2.0\n"
     ]
    }
   ],
   "source": [
    "# Checking Python version\n",
    "print(\"- Python version: {}\".format(platform.python_version()))\n",
    "print(\"- Matplotlib version is: {}\".format(matplotlib.__version__))\n",
    "print(\"- Pillow version is: {}\".format(PIL.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Paths and Constants Summary\n",
    "\n",
    "This part of the code defines useful constants and paths.\n",
    "\n",
    "## 1. Defined Constants and Paths\n",
    "- `DATASET_PATH`: Specifies the path to the dataset folder named \"archives\".\n",
    "- `DATASET_ID`: Provides the unique identifier necessary for downloading the dataset using the Kaggle API.\n",
    "- `COVID_PATH`: Defines the path to the directory containing COVID images within the dataset folder.\n",
    "- `NON_COVID_PATH`: Defines the path to the directory containing non-COVID images within the dataset folder.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "466d12a807cbf965"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# PATHS AND SIMILAR\n",
    "DATASET_PATH = \"archives\"  \n",
    "DATASET_ID = \"plameneduardo/sarscov2-ctscan-dataset\"\n",
    "COVID_PATH = os.path.join(DATASET_PATH, \"COVID\")\n",
    "NON_COVID_PATH = os.path.join(DATASET_PATH, \"non-COVID\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T10:26:11.671179Z",
     "start_time": "2024-02-26T10:26:11.664896Z"
    }
   },
   "id": "1b28b3861eb5f84b",
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Download from Kaggle\n",
    "\n",
    "This part of the code checks if the dataset exists in the workspace and downloads it from Kaggle if it does not already exist.\n",
    "\n",
    "## 1. Checking Dataset Existence\n",
    "- The code uses the `os.path.exists()` function to check if the dataset is already present in the workspace.\n",
    "\n",
    "## 2. Downloading Dataset from Kaggle\n",
    "- If the dataset does not exist, the code proceeds to download it from Kaggle using the `kaggle.api.dataset_download_files()` function.\n",
    "- The `DATASET_ID` and `DATASET_PATH` variables specify the dataset to download and the location to save it, respectively.\n",
    "- The `unzip=True` parameter ensures that the downloaded dataset is unzipped after download.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d1890303dddf97f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Dataset already downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset if not exist in the workplace\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(\"\\n> Download the dataset from Kaggle...\")\n",
    "    # Download dataset and unzip it\n",
    "    kaggle.api.dataset_download_files(dataset=DATASET_ID, path=DATASET_PATH, quiet=False, unzip=True)\n",
    "else:\n",
    "    print(\"\\n> Dataset already downloaded.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T10:26:15.924042Z",
     "start_time": "2024-02-26T10:26:15.914892Z"
    }
   },
   "id": "c294b78fcbcece79",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Preprocessing Summary\n",
    "\n",
    "This code performs preprocessing tasks on a dataset, which includes counting files, checking for corrupted files, filtering out duplicates, and plotting duplicate images alongside their originals.\n",
    "\n",
    "## 1. Counting Files\n",
    "The `count_files` function counts the number of files with specified extensions in a specified directory. It takes the directory path and file extensions as inputs and returns the count of files.\n",
    "\n",
    "## 2. Checking for Corrupted Files\n",
    "The `corruption_filter` function checks the dataset for corrupted image files and provides an option to delete them. It iterates through all files, verifies their integrity using PIL's `Image` module, and removes corrupted files if requested.\n",
    "\n",
    "## 3. Finding and Handling Duplicates\n",
    "The `find_out_duplicate` function identifies duplicate images within the dataset. It computes the hash of each image and compares it with previous hashes to detect duplicates. If duplicates are found, it plots each pair of original and duplicate images side by side using Matplotlib. The user is prompted to decide whether to delete the duplicate images.\n",
    "\n",
    "## Usage\n",
    "1. The code begins with checking the dataset's file count before preprocessing tasks.\n",
    "2. It then checks for corrupted files and provides an option to delete them.\n",
    "3. Next, it identifies and handles duplicates within the COVID and non-COVID dataset subdirectories.\n",
    "4. Finally, it confirms the total file count after preprocessing.\n",
    "\n",
    "This code ensures the integrity and cleanliness of the dataset for further analysis or model training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56d795380475bce7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> CHECK THE DATASET\n",
      "\n",
      "> Checking the Number of file before performing Pre-processing Task...\n",
      "- Total Number of file: 2430\n",
      "- Number of file in COVID: 1215\n",
      "- Number of file in non-Covid: 1215\n",
      "\n",
      "> Checking for corrupted files...\n",
      "> No Corrupted File Found\n",
      "\n",
      "> Checking duplicates in COVID/...[current num. of file: 1215]\n",
      "> No duplicate images found.\n",
      "\n",
      "> Checking duplicates in non-COVID/...[current num. of file: 1215]\n",
      "> No duplicate images found.\n",
      "\n",
      "> Final check to confirm the total file count:\n",
      "- Total Number of file: 2430\n",
      "- Number of file in COVID: 1215\n",
      "- Number of file in non-Covid: 1215\n",
      "\n",
      "> DATASET CHECK COMPLETE!\n"
     ]
    }
   ],
   "source": [
    "# Count the number of files\n",
    "def count_files(file_path, extensions=\"jpg\"):\n",
    "    \"\"\"\n",
    "    Count the number of files with specified extensions in the specified directory.\n",
    "\n",
    "    Example: count_files(\"/path/to/directory\", extensions=[\"jpg\", \"png\"]) -> 12\n",
    "\n",
    "    :param file_path: (str) The path to the directory for which file count is required.\n",
    "    :param extensions: (list or None) List of file extensions to count. If None, count all files.\n",
    "\n",
    "    :return: (int) The number of files with specified extensions in the specified directory.\n",
    "    \"\"\"\n",
    "\n",
    "    if extensions is None:\n",
    "        extensions = ['']\n",
    "\n",
    "    counter = 0\n",
    "    with os.scandir(file_path) as entries:\n",
    "        for entry in entries:\n",
    "            if entry.is_file() and any(entry.name.lower().endswith(ext) for ext in extensions):\n",
    "                counter += 1\n",
    "\n",
    "    return counter\n",
    "\n",
    "\n",
    "# Just a helper function\n",
    "def print_file_counts():\n",
    "    \"\"\"\n",
    "    A helper function that pint information about the number of files inside the directory.\n",
    "    \"\"\"\n",
    "\n",
    "    count_covid = count_files(file_path=COVID_PATH)\n",
    "    count_non_covid = count_files(file_path=NON_COVID_PATH)\n",
    "\n",
    "    tot_number_file = count_covid + count_non_covid\n",
    "    print(\"- Total Number of file: {}\\n\".format(tot_number_file) +\n",
    "          \"- Number of file in COVID: {}\\n\".format(count_covid) +\n",
    "          \"- Number of file in non-Covid: {}\\n\".format(count_non_covid))\n",
    "\n",
    "# Check dataset: filter out possible corrupted files.\n",
    "def corruption_filter(dataset_path):\n",
    "    \"\"\"\n",
    "    Check dataset for corrupted files and delete them if requested.\n",
    "\n",
    "    :param dataset_path: The path to the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize\n",
    "    bad_files = []  # to store corrupted file\n",
    "\n",
    "    # Loop through all dataset subfolders\n",
    "    for dirpath, _, filenames in os.walk(dataset_path):\n",
    "\n",
    "        # Ensure we're processing a sub-folder level\n",
    "        if dirpath is not dataset_path:\n",
    "\n",
    "            # Loop through all files\n",
    "            for filename in filenames:\n",
    "                # Check the file extension\n",
    "                if filename.endswith(\"jpg\"):\n",
    "                    # Get the file path\n",
    "                    file_path = os.path.join(dirpath, filename)\n",
    "                    try:\n",
    "                        with Image.open(file_path) as image:\n",
    "                            image.verify()\n",
    "                    except UnidentifiedImageError:\n",
    "                        bad_files.append(file_path)\n",
    "                        print(\"\\n> There are {} corrupted files: {}\".format(len(bad_files), bad_files))\n",
    "\n",
    "    if len(bad_files) != 0:\n",
    "        doc_message = input(\"\\n> Do you want to delete these {} file? [Y/N]: \".format(len(bad_files)))\n",
    "        if doc_message.upper() == \"Y\":\n",
    "            for bad_file in bad_files:\n",
    "                # delete duplicate\n",
    "                os.remove(bad_file)\n",
    "                print(\"- {} Corrupted File Deleted Successfully!\".format(bad_file))\n",
    "\n",
    "            # Print count\n",
    "            print(\"\\n> Checking the Number of file after the application of the corruption filter:\")\n",
    "            print_file_counts()\n",
    "    else:\n",
    "        print(\"> No Corrupted File Found\")\n",
    "    \n",
    "    \n",
    "# Check dataset: control the presence of duplicate inside the training set\n",
    "def find_out_duplicate(dataset_path, hash_size):\n",
    "    \"\"\"\n",
    "    Find and delete Duplicates inside the training set\n",
    "\n",
    "    :param dataset_path: the path to dataset.\n",
    "    :param hash_size: images will be resized to a matrix with size by given value.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize\n",
    "    hashes = {}\n",
    "    originals = {}\n",
    "    duplicates = []\n",
    "\n",
    "    # loop through file\n",
    "    for file in os.listdir(dataset_path):\n",
    "        file_path = os.path.join(dataset_path, file)\n",
    "\n",
    "        with Image.open(file_path) as image:\n",
    "            tmp_hash = imagehash.average_hash(image, hash_size)\n",
    "\n",
    "            if tmp_hash in hashes:\n",
    "                print(\"- Duplicate [{}] found for Og. Image [{}]\".format(file, hashes[tmp_hash]))\n",
    "                duplicates.append(file)  # duplicate files \n",
    "                originals[file] = hashes[tmp_hash]  # original files\n",
    "            else:\n",
    "                hashes[tmp_hash] = file\n",
    "\n",
    "    if len(duplicates) != 0:\n",
    "        \n",
    "        fig, axs = plt.subplots(len(duplicates), 2, figsize=(10, 5 * len(duplicates)))\n",
    "\n",
    "        for idx, duplicate in enumerate(duplicates):\n",
    "            duplicate_path = os.path.join(dataset_path, duplicate)\n",
    "            original_path = os.path.join(dataset_path, originals[duplicate])\n",
    "\n",
    "            # Load images\n",
    "            duplicate_image = plt.imread(duplicate_path)\n",
    "            original_image = plt.imread(original_path)\n",
    "\n",
    "            # Plot side by side\n",
    "            axs[idx, 0].imshow(original_image)\n",
    "            axs[idx, 0].set_title('Original')\n",
    "            axs[idx, 0].axis('off')\n",
    "\n",
    "            axs[idx, 1].imshow(duplicate_image)\n",
    "            axs[idx, 1].set_title('Duplicate')\n",
    "            axs[idx, 1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        doc_message = input(\"\\n> Do you want to delete these {} duplicate images? [Y/N]: \".format(len(duplicates)))\n",
    "\n",
    "        if doc_message.upper() == \"Y\":\n",
    "            for duplicate in duplicates:\n",
    "                # Delete duplicate\n",
    "                os.remove(os.path.join(dataset_path, duplicate))\n",
    "                print(\"- {} Deleted Successfully!\".format(duplicate))\n",
    "    else:\n",
    "        print(\"> No duplicate images found.\")\n",
    "\n",
    "\n",
    "print(\"\\n> CHECK THE DATASET\")\n",
    "print(\"\\n> Checking the Number of file before performing Pre-processing Task...\")\n",
    "\n",
    "# Print count\n",
    "print_file_counts()\n",
    "\n",
    "# Check for corrupted file\n",
    "print(\"> Checking for corrupted files...\")\n",
    "corruption_filter(dataset_path=DATASET_PATH)\n",
    "\n",
    "    # Check for duplicates in the dataset: COVID/ and non-COVID/\n",
    "print(\"\\n> Checking duplicates in COVID/...[current num. of file: {}]\"\n",
    "      .format(count_files(file_path=COVID_PATH)))\n",
    "find_out_duplicate(dataset_path=COVID_PATH, hash_size=8)\n",
    "\n",
    "print(\"\\n> Checking duplicates in non-COVID/...[current num. of file: {}]\"\n",
    "      .format(count_files(file_path=NON_COVID_PATH)))\n",
    "find_out_duplicate(dataset_path=NON_COVID_PATH, hash_size=8)\n",
    "\n",
    "print(\"\\n> Final check to confirm the total file count:\")\n",
    "print_file_counts()\n",
    "\n",
    "print(\"> DATASET CHECK COMPLETE!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-26T10:48:02.993822Z",
     "start_time": "2024-02-26T10:47:54.496365Z"
    }
   },
   "id": "b4094d142f206980",
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "15095289fc0d309e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
